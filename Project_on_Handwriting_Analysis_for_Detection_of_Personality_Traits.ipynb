{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project on  Handwriting-Analysis-for-Detection-of-Personality-Traits.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP6PUyzNdcXTtk67g/2nMFL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mayukbasu/Handwriting-Analysis-for-Detection-of-Personality-Traits/blob/main/Project_on_Handwriting_Analysis_for_Detection_of_Personality_Traits.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5Y4DBlmYivP"
      },
      "source": [
        "from __future__ import print_function\n",
        "# coding: utf-8\n",
        "\n",
        "######################################################################################\n",
        "# # TRAIN:\n",
        "import os\n",
        "import cv2\n",
        "# simplified interface for building models\n",
        "import keras\n",
        "import pickle\n",
        "import numpy as np\n",
        "import variables as vars\n",
        "import matplotlib.pyplot as plt\n",
        "# because our models are simple\n",
        "from keras.models import Sequential\n",
        "from keras.models import model_from_json\n",
        "# for convolution (images) and pooling is a technique to help choose the most relevant features in an image\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from scipy.misc import imread, imresize, imshow\n",
        "# dense means fully connected layers, dropout is a technique to improve convergence, flatten to reshape our matrices for feeding\n",
        "# into respective layers\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "img_rows, img_cols = vars.img_rows, vars.img_cols\n",
        "batch_size = vars.batch_size\n",
        "num_classes = vars.num_classes\n",
        "epochs = vars.epochs\n",
        "model_json_path = vars.model_json_path\n",
        "model_path = vars.model_path\n",
        "prediction_file_dir_path = vars.prediction_file_dir_path\n",
        "\n",
        "path = 'FEATURE-BASED-IMAGES/'\n",
        "\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "\n",
        "for folder, subfolders, files in os.walk(path):\n",
        "  for name in files:\n",
        "    if name.endswith('.jpg'):\n",
        "      x = cv2.imread(folder + '/' + name, cv2.IMREAD_GRAYSCALE)\n",
        "      x = imresize(x, (img_rows, img_cols))\n",
        "      __, x = cv2.threshold(x, 220, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "      # dilate\n",
        "      morph_size = (2, 2)\n",
        "      cpy = x.copy()\n",
        "      struct = cv2.getStructuringElement(cv2.MORPH_RECT, morph_size)\n",
        "      cpy = cv2.dilate(~cpy, struct, anchor=(-1, -1), iterations=1)\n",
        "      x = ~cpy\n",
        "\n",
        "      x = np.expand_dims(x, axis=4)\n",
        "\n",
        "      data.append(x)\n",
        "\n",
        "      # cv2.imwrite(str(name) + '00986.jpg', x)\n",
        "      labels.append(os.path.basename(folder))\n",
        "\n",
        "data1 = np.asarray(data)\n",
        "labels1 = np.asarray(labels)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(data1, labels1,\n",
        "                                                    random_state=0,\n",
        "                                                    test_size=0.5\n",
        "                                                    )\n",
        "x_train = np.array(x_train)\n",
        "x_test = np.array(x_test)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "input_shape = (img_rows, img_cols, 1)\n",
        "# build our model\n",
        "model = Sequential()\n",
        "# convolutional layer with rectified linear unit activation\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "# again\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "# choose the best features via pooling\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "# randomly turn neurons on and off to improve convergence\n",
        "model.add(Dropout(0.25))\n",
        "# flatten since too many dimensions, we only want a classification output\n",
        "model.add(Flatten())\n",
        "# fully connected to get all relevant data\n",
        "model.add(Dense(128, activation='relu'))\n",
        "# one more dropout for convergence' sake :)\n",
        "model.add(Dropout(0.5))\n",
        "# output a softmax to squash the matrix into output probabilities\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "# Adaptive learning rate (adaDelta) is a popular form of gradient descent rivaled only by adam and adagrad\n",
        "# categorical ce since we have multiple classes (10)\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "# train that ish!\n",
        "\n",
        "lb = LabelEncoder()\n",
        "y_train = lb.fit_transform(y_train)\n",
        "y_test = lb.fit_transform(y_test)\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train)\n",
        "y_test = keras.utils.to_categorical(y_test)\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOBA0dLvYzyK"
      },
      "source": [
        "# Save the model\n",
        "# serialize model to JSON\n",
        "model_json = model.to_json()\n",
        "with open(model_json_path, \"w\") as json_file:\n",
        "  json_file.write(model_json)\n",
        "\n",
        "# pickle label encoder obj\n",
        "with open(vars.label_obj_path, 'wb') as lb_obj:\n",
        "  pickle.dump(lb, lb_obj)\n",
        "\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(model_path)\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFmSRDstZAeV"
      },
      "source": [
        "from __future__ import print_function\n",
        "# coding: utf-8\n",
        "\n",
        "######################################################################################\n",
        "# # TRAIN:\n",
        "import os\n",
        "import cv2\n",
        "# simplified interface for building models\n",
        "import keras\n",
        "import pickle\n",
        "import numpy as np\n",
        "import variables as vars\n",
        "import matplotlib.pyplot as plt\n",
        "# because our models are simple\n",
        "from keras.models import Sequential\n",
        "from keras.models import model_from_json\n",
        "# for convolution (images) and pooling is a technique to help choose the most relevant features in an image\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from scipy.misc import imread, imresize, imshow\n",
        "# dense means fully connected layers, dropout is a technique to improve convergence, flatten to reshape our matrices for feeding\n",
        "# into respective layers\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "img_rows, img_cols = vars.img_rows, vars.img_cols\n",
        "batch_size = vars.batch_size\n",
        "num_classes = vars.num_classes\n",
        "epochs = vars.epochs\n",
        "model_json_path = vars.model_json_path\n",
        "model_path = vars.model_path\n",
        "prediction_file_dir_path = vars.prediction_file_dir_path\n",
        "\n",
        "###############################################################################\n",
        "# PREDICTION\n",
        "#############################################################################@##\n",
        "\n",
        "\n",
        "def print_results(class_lbl, out):\n",
        "  print('\\n', '~' * 60)\n",
        "  for k, lbl in enumerate(class_lbl):\n",
        "    if lbl == 'LEFT_MARG':\n",
        "      print('\\n > Courageous :', '\\t' * 5, out[k] * 100, '%')\n",
        "      print('\\n > Insecure and devotes oneself completely :\\t',\n",
        "            100 - (out[k] * 100), '%')\n",
        "    elif lbl == 'RIGHT_MARG':\n",
        "      print('\\n > Avoids future and a reserved person :\\t', out[k] * 100, '%')\n",
        "    elif lbl == 'SLANT_ASC':\n",
        "      print('\\n > Optimistic :', '\\t' * 5, out[k] * 100, '%')\n",
        "    elif lbl == 'SLANT_DESC':\n",
        "      print('\\n > Pessimistic :', '\\t' * 4, out[k] * 100, '%')\n",
        "  print('~' * 60, '\\n')\n",
        "\n",
        "\n",
        "def predict_personalities(filename):\n",
        "\n",
        "  try:\n",
        "    json_file = open(model_json_path, 'r')\n",
        "    loaded_model_json = json_file.read()\n",
        "    json_file.close()\n",
        "\n",
        "    from keras.models import model_from_json\n",
        "    loaded_model = model_from_json(\n",
        "        open(\n",
        "            model_json_path).read())\n",
        "    # load woeights into new model\n",
        "    loaded_model.load_weights(model_path)\n",
        "    print(\"*****Loaded Model from disk******\")\n",
        "  except Exception:\n",
        "    return '\\n\\n> Need to train the model first!\\n'\n",
        "  x = cv2.imread(prediction_file_dir_path + filename, cv2.IMREAD_GRAYSCALE)\n",
        "  x = imresize(x, (img_rows, img_cols))\n",
        "  __, x = cv2.threshold(x, 220, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "  # dilate\n",
        "  morph_size = (2, 2)\n",
        "  cpy = x.copy()\n",
        "  struct = cv2.getStructuringElement(cv2.MORPH_RECT, morph_size)\n",
        "  cpy = cv2.dilate(~cpy, struct, anchor=(-1, -1), iterations=1)\n",
        "  x = ~cpy\n",
        "  x = np.expand_dims(x, axis=4)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  out = loaded_model.predict(x, batch_size=32, verbose=0)\n",
        "\n",
        "  with open(vars.label_obj_path, 'rb') as lb_obj:\n",
        "    lb = pickle.load(lb_obj)\n",
        "\n",
        "  result = lb.inverse_transform(np.argmax(out[0]))\n",
        "  print_results(lb.classes_, out[0])\n",
        "\n",
        "  return '\\n> Prediction Completed!'\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  fpath = None\n",
        "  for dir_0, sub_dir_0, files in os.walk(prediction_file_dir_path):\n",
        "    fpath = files\n",
        "    break\n",
        "  if fpath:\n",
        "    res = predict_personalities(fpath[0])\n",
        "    print(res)\n",
        "  else:\n",
        "    print('No file found for prediction!')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}